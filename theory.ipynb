{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Theory\n",
    "\n",
    "This notebook discusses theoretical aspects of the network.\n",
    "\n",
    "## Separable Convolutions\n",
    "\n",
    "The bottleneck and downsampling blocks make use of separable \n",
    "convolutions in order to improve performance. A separable convolution\n",
    "involves breaking a standard CNN-style 2D convolution into two separate\n",
    "convolutions, one for spatial mixing and one for channel mixing. \n",
    "In the literature spatial mixing is often called a depthwise convolution \n",
    "while channel mixing is called a pointwise convolution. \n",
    "For the rest of this document, subscripts $d$ and $p$ will be used to denote\n",
    "depthwise and pointwise CNN-style 2D convolutions respectively.\n",
    "\n",
    "Let\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "    N_o = \\text{number of input filters} \\\\\n",
    "    N_i = \\text{number of output filters} \\\\\n",
    "    L_r = \\text{input rows} \\\\\n",
    "    L_c = \\text{input columns} \\\\\n",
    "    F_r = \\text{filter rows} \\\\\n",
    "    F_c = \\text{filter columns} \\\\\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Under a standard CNN-style 2D convolution (assume `padding=same`), we have\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "    \\text{weights} = N_o N_i F_r F_c \n",
    "    &&\n",
    "    \\text{MACs} = N_o N_i F_r F_c L_r L_c\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "By splitting CNN-style 2D convolution into spatial and depthwise components \n",
    "we instead have two separate convolutions\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "    \\text{weights}_d = N_i F_r F_c\n",
    "    &&\n",
    "    \\text{weights}_p = N_o N_i \n",
    "    \\\\\n",
    "    \\text{MACs}_d = N_i F_r F_c L_r L_c \\\\\n",
    "    &&\n",
    "    \\text{MACs}_p = N_o N_i L_r L_c \n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "The total weights and MACs for both convolutions is\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "    \\text{weights}_{tot} = N_i (N_o + F_r F_c)\n",
    "    &&\n",
    "    \\text{MACs}_{tot} = N_i L_r L_c (N_o + F_r F_c)\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Finally, in order to see an improvement in memory / MACs we must \n",
    "satisfy the following inequalities \n",
    "(which quickly simlify to the same inequality).\n",
    "\n",
    "For memory:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "    N_o N_i F_r F_c &\\geq N_i (N_o + F_r F_c) \\\\\n",
    "    N_o F_r F_c &\\geq N_o + F_r F_c \\\\\n",
    "    N_o F_r F_c &\\geq N_o + F_r F_c\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "For MACs:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "    N_o N_i F_r F_c L_r L_c &\\geq N_i L_r L_c (N_o + F_r F_c) \\\\\n",
    "    N_o F_r F_c &\\geq N_o + F_r F_c \\\\\n",
    "    N_o F_r F_c &\\geq N_o + F_r F_c\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "If we assume a resonable kernel size of $F_r = F_c = 3$ we have\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "    9 N_o &\\geq N_o + 9 \\\\\n",
    "    N_o &\\geq \\frac{9}{8} \\\\\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "This confirms that for reasonable parameters we will see a reduction in \n",
    "memory / compute with separable convolutions while still mixing information\n",
    "in both spatial and channel dimensions.\n",
    "\n",
    "## Generalized Separable Convolutions\n",
    "\n",
    "The case mentioned above dealt specifically with depthwise convolutions such that\n",
    "$N_o = N_i$. Now consider the general case of depthwise convolutions \n",
    "such that $\\gamma$ output feature maps are produced for each input feature map,\n",
    "i.e. $N_o = \\gamma N_i$. We will still permit a $N_o \\rightarrow N_i$ mapping in \n",
    "the pointwise step, but note that this calculation could be simplified under the \n",
    "constraint that the pointwise $N_i = N_o$ ($N_o$ would be expressed in terms of $\\gamma$).\n",
    "\n",
    "Assuming that the output of the depthwise convolution is given as input to \n",
    "the pointwise convolution, we have\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "    \\text{weights}_d = \\gamma N_i F_r F_c\n",
    "    &&\n",
    "    \\text{weights}_p = \\gamma N_o N_i\n",
    "    \\\\\n",
    "    \\text{MACs}_d = \\gamma N_i F_r F_c L_r L_c \\\\\n",
    "    &&\n",
    "    \\text{MACs}_p = \\gamma N_o N_i L_r L_c \n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "We must have $\\gamma \\geq 1$, as a reduction in channels with a depthwise \n",
    "convolution could only be achieved by throwing away input feature maps.\n",
    "In light of this, it would appear that multiplying depth at the depthwise\n",
    "convolution stage will increase the memory and compute requirements of the \n",
    "network. \n",
    "\n",
    "Instead, let us consider the effect of distributing a desired depth \n",
    "multiplication over both the depthwise and pointwise steps. Suppose that the\n",
    "target depth multiplication is $\\gamma$, i.e. $N_o = \\gamma N_i$. We will perform\n",
    "a $\\sqrt{\\gamma}$ multiplication at the depthwise convolution and a $\\sqrt{\\gamma}$ \n",
    "multiplication at the pointwise convolution.\n",
    "\n",
    "The depthwise step will produce $\\sqrt{\\gamma} N_i$ feature maps that will be \n",
    "fed as input to the pointwise step. For clarity, the pointwise step will now have\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "    N_i = \\sqrt{\\gamma} N_i^{\\text{depthwise} }\n",
    "    &&\n",
    "    N_o = \\sqrt{\\gamma} N_i = \\gamma N_i^{\\text{depthwise}}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "This gives \n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "    \\text{weights}_d = \\sqrt{\\gamma} N_i F_r F_c\n",
    "    &&\n",
    "    \\text{weights}_p = \\gamma^\\frac{3}{2} N_i^2\n",
    "    \\\\\n",
    "    \\text{MACs}_d = \\sqrt{\\gamma} N_i F_r F_c L_r L_c \\\\\n",
    "    &&\n",
    "    \\text{MACs}_p = \\gamma^\\frac{3}{2} N_i^2 L_r L_c \n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "The total weights and MACs for both convolutions is\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "    \\text{weights}_{tot} &= \\sqrt{\\gamma} N_i (N_i \\gamma + F_r F_c) \\\\\n",
    "    \\text{MACs}_{tot} &= \\sqrt{\\gamma} N_i L_r L_c (N_i \\gamma + F_r F_c)\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Recall that for unit depthwise multiplication we had\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "    \\text{weights}_{tot} &= N_i (N_o + F_r F_c) \\\\\n",
    "    \\text{MACs}_{tot} &= N_i L_r L_c (N_o + F_r F_c)\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Under the constraint of $\\gamma \\geq 1$ it is clear that distributing the depth\n",
    "multiplication across the depthwise and pointwise steps does not improve\n",
    "memory or compute. This strategy may be helpful in the tail where a significant\n",
    "increase in depth is needed over a short number of convolutional layers. \n",
    "Rather than force a mapping of $N_i$ input channels to $\\gamma N_i$ output \n",
    "channels directly, the distributed approach creates $\\sqrt{\\gamma N_i}$ weak \n",
    "features on a per-channel basis which are then mapped to $\\gamma N_i$ output \n",
    "features."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
